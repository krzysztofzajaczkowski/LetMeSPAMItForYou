{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Analise and preprocessed data before filtering to choose best model.\n",
    "Sections:\n",
    "- #1 print functions\n",
    "- #2 load data and init values\n",
    "- #3 define functions to processed\n",
    "- #4 processed and collect various data\n",
    "- #5 print data to analise\n",
    "- #6 Save data to files\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def print_unique_words_number():\n",
    "    print(\"Number of unique words in {0} dataset: {1}\".format(1, len(Counter(uq_dict1))))\n",
    "    print(\"Number of unique words in {0} spam dataset: {1}\".format(1, len(Counter(uq_dict1_spam))))\n",
    "    print(\"Number of unique words in {0} ham dataset: {1}\".format(1, len(Counter(uq_dict1_ham))))\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Number of unique words in {0} dataset: {1}\".format(2, len(Counter(dict2))))\n",
    "    print(\"Number of unique words in {0} spam dataset: {1}\".format(2, len(Counter(dict2_spam))))\n",
    "    print(\"Number of unique words in {0} ham dataset: {1}\".format(2, len(Counter(dict2_ham))))\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "\n",
    "def print_loaded_data():\n",
    "    print(\"First dataset form {0}: \\n {1}\\n\\n\\n\".format(PATH_DATASET1, df_dataset1))\n",
    "    print(\"Spam from first dataset:\\n {0}\\n\\n\\n\".format(df_dataset1_spam))\n",
    "    print(\"Nonspam from first dataset:\\n {0}\\n\\n\\n\".format(df_dataset1_ham))\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Second dataset form {0}: \\n {1}\\n\\n\\n\".format(PATH_DATASET2, df_dataset2))\n",
    "    print(\"Spam from second dataset:\\n {0}\\n\\n\\n\".format(df_dataset2_spam))\n",
    "    print(\"Nonspam from second dataset:\\n {0}\\n\\n\\n\".format(df_dataset2_ham))\n",
    "    print(\"------------------------------------------------------------------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% --- PRINT FUNCTIONS\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% --- INIT\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import  pandas as pd\n",
    "import  numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Consts\n",
    "PATH_DATASET1 = \"../data/raw/spamham.csv\"\n",
    "PATH_DATASET2 = \"../data/raw/spam_or_not_spam.csv\"\n",
    "HEADER_NAMES = [\"text\", \"is_spam\"]\n",
    "\n",
    "# Load data to dataframe\n",
    "df_dataset1 = pd.read_csv(PATH_DATASET1, names=HEADER_NAMES, header=None).dropna()\n",
    "df_dataset2 = pd.read_csv(PATH_DATASET2, names=HEADER_NAMES, header=None).dropna()\n",
    "\n",
    "# Load spam and non spam dataframes\n",
    "df_dataset1_spam = df_dataset1[df_dataset1[HEADER_NAMES[1]] == 1]\n",
    "df_dataset1_ham = df_dataset1[df_dataset1[HEADER_NAMES[1]] == 0]\n",
    "\n",
    "df_dataset2_spam = df_dataset2[df_dataset2[HEADER_NAMES[1]] == 1]\n",
    "df_dataset2_ham = df_dataset2[df_dataset2[HEADER_NAMES[1]] == 0]\n",
    "\n",
    "# Print data\n",
    "#print_loaded_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def create_dict_unique(df_org, char=' '):\n",
    "    \"\"\"\n",
    "    Split text data to numpy array and create column with unique word dictionary\n",
    "\n",
    "    :param df_org: dataframe with text column\n",
    "    :param char: split text by this character\n",
    "    :return: dataframe with dict and splited text\n",
    "    \"\"\"\n",
    "\n",
    "    df = df_org.copy()\n",
    "    # split by char\n",
    "    df[HEADER_NAMES[0]] = df[HEADER_NAMES[0]].apply(lambda text: np.array(text.split(char)))\n",
    "    df[\"dict\"] = df[HEADER_NAMES[0]].apply(\n",
    "        lambda text: dict(zip(np.unique(text, return_counts=True)[0], [1]*len(np.unique(text)))))\n",
    "    return df\n",
    "\n",
    "def create_dict_counts(df_org, char=' '):\n",
    "    \"\"\"\n",
    "    Split text data to numpy array and create column with words count dictionary\n",
    "\n",
    "    :param df_org: dataframe with text column\n",
    "    :param char: split text by this character\n",
    "    :return: dataframe with dict and splited text\n",
    "    \"\"\"\n",
    "\n",
    "    df = df_org.copy()\n",
    "    # split by char\n",
    "    df[HEADER_NAMES[0]] = df[HEADER_NAMES[0]].apply(lambda text: np.array(text.split(char)))\n",
    "    df[\"dict\"] = df[HEADER_NAMES[0]].apply(\n",
    "        lambda text: dict(zip(np.unique(text, return_counts=True)[0],\n",
    "                              np.unique(text, return_counts=True)[1])))\n",
    "    return df\n",
    "\n",
    "def concat_dict(df):\n",
    "    \"\"\"\n",
    "    Calc dict with add up all words from dataframe\n",
    "\n",
    "    :param df: dataframe with dict column\n",
    "    :return: return merged dict with all words and their count\n",
    "    \"\"\"\n",
    "    return dict((df[\"dict\"].apply(lambda x: Counter(x))).sum())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% --- FUNCTONS\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# # COUNT\n",
    "# Load dataframes with counted word in mail\n",
    "df_dict1 = create_dict_counts(df_dataset1)\n",
    "df_dict1_spam = create_dict_counts(df_dataset1_spam)\n",
    "df_dict1_ham = create_dict_counts(df_dataset1_ham)\n",
    "\n",
    "df_dict2 = create_dict_counts(df_dataset2)\n",
    "df_dict2_spam = create_dict_counts(df_dataset2_spam)\n",
    "df_dict2_ham = create_dict_counts(df_dataset2_ham)\n",
    "\n",
    "# Load dicts with sum\n",
    "dict1 = concat_dict(df_dict1)\n",
    "dict1_spam = concat_dict(df_dict1_spam)\n",
    "dict1_ham = concat_dict(df_dict1_ham)\n",
    "\n",
    "dict2 = concat_dict(df_dict2)\n",
    "dict2_spam = concat_dict(df_dict2_spam)\n",
    "dict2_ham = concat_dict(df_dict2_ham)\n",
    "\n",
    "# Save words in ascending order\n",
    "max_values1 = Counter(dict1).most_common(len(Counter(dict1)))\n",
    "max_values1_spam = Counter(dict1_spam).most_common(len(Counter(dict1_spam)))\n",
    "max_values1_ham = Counter(dict1_ham).most_common(len(Counter(dict1_ham)))\n",
    "\n",
    "max_values2 = Counter(dict2).most_common(len(Counter(dict2)))\n",
    "max_values2_spam = Counter(dict2_spam).most_common(len(Counter(dict2_spam)))\n",
    "max_values2_ham = Counter(dict2_ham).most_common(len(Counter(dict2_ham)))\n",
    "\n",
    "# # UNIQUE\n",
    "# Load dataframes with unique word in mail\n",
    "uq_df_dict1 = create_dict_unique(df_dataset1)\n",
    "uq_df_dict1_spam = create_dict_unique(df_dataset1_spam)\n",
    "uq_df_dict1_ham = create_dict_unique(df_dataset1_ham)\n",
    "\n",
    "uq_df_dict2 = create_dict_unique(df_dataset2)\n",
    "uq_df_dict2_spam = create_dict_unique(df_dataset2_spam)\n",
    "uq_df_dict2_ham = create_dict_unique(df_dataset2_ham)\n",
    "\n",
    "# Load dicts with sum\n",
    "uq_dict1 = concat_dict(uq_df_dict1)\n",
    "uq_dict1_spam = concat_dict(uq_df_dict1_spam)\n",
    "uq_dict1_ham = concat_dict(uq_df_dict1_ham)\n",
    "\n",
    "uq_dict2 = concat_dict(uq_df_dict2)\n",
    "uq_dict2_spam = concat_dict(uq_df_dict2_spam)\n",
    "uq_dict2_ham = concat_dict(uq_df_dict2_ham)\n",
    "\n",
    "# Save words in ascending order\n",
    "uq_max_values1 = Counter(uq_dict1).most_common(len(Counter(uq_dict1)))\n",
    "uq_max_values1_spam = Counter(uq_dict1_spam).most_common(len(Counter(uq_dict1_spam)))\n",
    "uq_max_values1_ham = Counter(uq_dict1_ham).most_common(len(Counter(uq_dict1_ham)))\n",
    "\n",
    "uq_max_values2 = Counter(uq_dict2).most_common(len(Counter(uq_dict2)))\n",
    "uq_max_values2_spam = Counter(uq_dict2_spam).most_common(len(Counter(uq_dict2_spam)))\n",
    "uq_max_values2_ham = Counter(uq_dict2_ham).most_common(len(Counter(uq_dict2_ham)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% --- LOAD AND COLLECT DICTS\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print_words_number\n",
      "Number of unique words in 1 dataset: 23111\n",
      "Number of unique words in 1 spam dataset: 11261\n",
      "Number of unique words in 1 ham dataset: 16868\n",
      "------------------------------------------------------------------------\n",
      "Number of unique words in 2 dataset: 21040\n",
      "Number of unique words in 2 spam dataset: 10968\n",
      "Number of unique words in 2 ham dataset: 13890\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print number of words\n",
    "print(\"print_words_number\")\n",
    "print_unique_words_number()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% --- ANALYSIS\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# paths to save file\n",
    "UQ_PATH_SAVE_DATASET1 = \"../data/processed/unique_dataset1.csv\"\n",
    "UQ_PATH_SAVE_DATASET1_SPAM = \"../data/processed/unique_dataset1_spam.csv\"\n",
    "UQ_PATH_SAVE_DATASET1_HAM = \"../data/processed/unique_dataset1_ham.csv\"\n",
    "\n",
    "UQ_PATH_SAVE_DATASET2 = \"../data/processed/unique_dataset2.csv\"\n",
    "UQ_PATH_SAVE_DATASET2_SPAM = \"../data/processed/unique_dataset2_spam.csv\"\n",
    "UQ_PATH_SAVE_DATASET2_HAM = \"../data/processed/unique_dataset2_ham.csv\"\n",
    "\n",
    "# prepare files to save\n",
    "df_save1 = pd.DataFrame(uq_max_values1)\n",
    "df_save1_spam = pd.DataFrame(uq_max_values1_spam)\n",
    "df_save1_ham = pd.DataFrame(uq_max_values1_ham)\n",
    "\n",
    "# header of file: NUMBER OF UNIQUE WORDS, EMAILS\n",
    "df_save1.to_csv(UQ_PATH_SAVE_DATASET1, index=False,\n",
    "                    header=[str(len(Counter(uq_dict1))), str(len(uq_df_dict1[\"dict\"]))])\n",
    "df_save1_spam.to_csv(UQ_PATH_SAVE_DATASET1_SPAM, index=False,\n",
    "                    header=[str(len(Counter(uq_dict1_spam))), str(len(uq_df_dict1_spam[\"dict\"]))])\n",
    "df_save1_ham.to_csv(UQ_PATH_SAVE_DATASET1_HAM, index=False,\n",
    "                    header=[str(len(Counter(uq_dict1_ham))), str(len(uq_df_dict1_ham[\"dict\"]))])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% ---  SAVE TO CSV\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}